# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**
This project uses the UCI Bank Marketing Dataset that contains data about banking clients, which include personal details of clients like age, job, marital status etc and details regarding maketing campaigning.
We seek to predict whether the client will subscribe a term deposit or not, hence making it a binary classification problem with two classes - 'yes' and 'no'.

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**
The project consists of two methods:
Method 1: Train a Scikit-learn Logistic Regression model with optmized hyperparameter tuning using HyperDrive
Method 2: Use AutoML to build and optimize a model
The results of the above methods are then compared.

<architecture-image>

The best performing model was the VotingEnsemble model selected through AutoML, which gave an accuracy of 0.91767. 
The Logistic Regression model with hyperparameters selected through HyperDrive gave an accuracy of 9.90996


## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

<architecture-image>

Data Preparation: A TabularDataset is created using Tabular DatasetFactory. The data is then cleaned and one hot encoded, and finally split into train and test sets. 

Code Scikit-learn Logistic Regression Model: The Logistic regression model is created. The model requires two hyperparameters: C, which is the inverse of regularization strength and max-iter which is the maximum number of iterations to converge. The Logistic Regression classificatio algorithm uses a sigmoid function to model the probability of a set of binary classes (yes/no). In any Machine Learning model, there is a chance of overfitting, which is a phenomenon where the model becomes 'too comfortable' with the training data that it does not generalize well. Regularization combats overfitting by making the model coefficients smaller. Thus larger C means less regularization and smaller C means more regularization. 
Accuracy is the primary metric here and that is saved in the run log.

HyperDrive Configuration: HyperDrive is configured with the estimator (the file with the coded model), parameter sampler, early termination policy, primary metric name & goal, maximum total runs and maximum concurrent runs. The Hyperdrive run is then submitted to the experiment. Once the run is completed, the best metrics are obtained and the model from the best run is registered.

Save Model: Once the run is completed, the best metrics are obtained and the model from the best run is registered.

**What are the benefits of the parameter sampler you chose?**
The parameter sampling method chosen for this experimnent is Random Sampling. Randing Sampling supports both discrete and continuous hyperparameters. In Logistic Regression, there are two hyperparameters:
i) inverse of regularization strength which is treated as continuous
ii) maximum number of iterations, which is treated as discrete

Random sampling also supports early termination of low-performance runs. For each of the hyperparameters, hyperparameter space is defined. In this experiment,
i) C (inversion of regularization strength)
    uniform (0.01, 1.00)
    This returns values uniformly distributed between 0.01 and 1.00
ii) max-iter (maximum number of iterations)
    choice (100, 200, 300, 400, 500)
    This returns a value chosen among given discrete values 100, 200, 300, 400, 500
Hyperparameter values are randomly selected from the defined search space.


**What are the benefits of the early stopping policy you chose?**
Automatically terminating poorly performing runs with an early termination policy improves computational efficiency.
In this experiment, Bandit policy is used. Bandit policy is based on slack factor/slack amount and evaluation interval. Bandit terminates runs where the primary metric is not within the specific slack factor/slack amount comapred to the best performing run. 
The primary metric for the experiment is accuracy.

The following configuration parameters were specified:
i) evaluation_interval=2
    The policy is applied every other time the training script logs the primary metric.
ii) delay_evaluation=5
    The first policy evaluation is delayed to avoid premature termination.
iii) slack_factor=0.1
    Any run whose best metric run is less than (1/(1+0.1)) or 91% of the best performing run will be terminated. 

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

The VotingEnsemble model defines an ensemble created from previous AutoML iterations that implements soft voting. The hyperparameters generated are the classifiers for the ensemble and the weights. 

weights=[0.4666666666666667, 0.2, 0.06666666666666667, 0.06666666666666667, 0.06666666666666667, 0.06666666666666667, 0.06666666666666667]
                                                        
                                                        
                                                        
                                                        
                                                    

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
